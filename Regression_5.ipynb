{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c824c8",
   "metadata": {},
   "source": [
    "#Ans1) \n",
    "\n",
    "Elastic Net Regression is a linear regression technique that combines Lasso (L1 regularization) and Ridge (L2 regularization) regression penalties in a single model. It differs from other regression techniques by addressing some of the limitations of Lasso and Ridge regression alone. While Lasso tends to select only a subset of features by setting some coefficients to zero, and Ridge regression shrinks the coefficients towards zero, Elastic Net combines both penalties to balance between feature selection and coefficient shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddd058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e63790fc",
   "metadata": {},
   "source": [
    "#Ans2.) \n",
    "\n",
    "The optimal values of the regularization parameters (alpha and l1_ratio) for Elastic Net Regression can be chosen using techniques like cross-validation. Grid search or randomized search can be employed to search through a range of parameter values and select the combination that minimizes a chosen evaluation metric, such as mean squared error (MSE) or cross-validated performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1b340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e512853",
   "metadata": {},
   "source": [
    "#Ans3.) \n",
    "\n",
    "Advantages:\n",
    "\n",
    "Handles multicollinearity well by shrinking and selecting groups of correlated variables together.\n",
    "Allows for both feature selection and coefficient shrinkage.\n",
    "More stable when the number of predictors is greater than the number of observations compared to Lasso.\n",
    "Suitable for situations where there are many correlated predictors.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Requires tuning of hyperparameters (alpha and l1_ratio).\n",
    "Computationally more expensive than Lasso or Ridge regression.\n",
    "May be less interpretable compared to simpler models like linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e590d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5aae703",
   "metadata": {},
   "source": [
    "#Ans4.) \n",
    "\n",
    "Predictive modeling in situations with high-dimensional data and multicollinearity.\n",
    "Financial modeling to predict stock prices or risk analysis.\n",
    "Healthcare applications for predicting patient outcomes based on multiple correlated predictors.\n",
    "Marketing analytics to understand customer behavior and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48f8e4d2",
   "metadata": {},
   "source": [
    "#Ans5.) \n",
    "\n",
    "The coefficients in Elastic Net Regression represent the change in the target variable for a one-unit change in the predictor variable, holding all other variables constant. A positive coefficient indicates that an increase in the predictor variable leads to an increase in the target variable, while a negative coefficient indicates the opposite. The magnitude of the coefficient reflects the strength of the relationship between the predictor and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a00e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5ea36ce",
   "metadata": {},
   "source": [
    "#Ans6.)\n",
    "\n",
    "Missing values in the dataset can be handled before applying Elastic Net Regression by methods such as imputation (replacing missing values with estimated values), removing observations with missing values, or using algorithms that can handle missing values internally. Imputation methods include mean or median imputation, predictive imputation using regression models, or advanced techniques like K-nearest neighbors imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a26c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57a87c58",
   "metadata": {},
   "source": [
    "#Ans7.) \n",
    "\n",
    "Elastic Net Regression performs feature selection by setting some coefficients to zero, effectively removing those features from the model. The l1_ratio parameter controls the balance between L1 (Lasso) and L2 (Ridge) penalties, where higher values of l1_ratio result in more features with zero coefficients. By tuning the regularization parameters alpha and l1_ratio appropriately, Elastic Net can automatically select relevant features while shrinking the coefficients of less important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad719dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ef8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans8.) \n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Train your Elastic Net Regression model\n",
    "model = ElasticNet()\n",
    "# ... Train the model\n",
    "\n",
    "# Pickle the trained model\n",
    "with open('elastic_net_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Unpickle the trained model\n",
    "with open('elastic_net_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cc9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a55eb06",
   "metadata": {},
   "source": [
    "#Ans9.) \n",
    "\n",
    "Pickling a model allows you to save the trained model object to a file, which can then be reused later without needing to retrain the model. This is useful for deploying the model in production environments, sharing the model with others, or storing the model for future use. Pickling preserves the state of the model, including its architecture, parameters, and trained weights, allowing you to load and use it as if it were still in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce3d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd6646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
